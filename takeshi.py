# ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼

# ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€â­ï¸ãƒ‡ãƒ¼ã‚¿è§£æã‚·ã‚¹ãƒ†ãƒ  æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€ŒHanakoğŸ˜¸ã€â­ï¸

# ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼

# =======================ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è§£æå¯¾å¿œã€‘=======================
# æœ¬ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€è¤‡æ•°ç¨®é¡ã®ç”Ÿä½“æƒ…å ±ï¼ˆãƒ¢ãƒ€ãƒªãƒ†ã‚£ï¼‰ã‚’çµ„ã¿åˆã‚ã›ã¦ã€
# SVRãƒ¢ãƒ‡ãƒ«ï¼ˆSupport Vector Regressionï¼‰ã«ã‚ˆã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è§£æã‚’å®Ÿç¾ã€‚
#
# å…¥åŠ›ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ã¯ä»¥ä¸‹ã‚’å«ã‚€ï¼š
#  - ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«ãƒ‡ãƒã‚¤ã‚¹ç”±æ¥ã®ç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ï¼ˆå¿ƒæ‹ãƒ»æ­©æ•°ãƒ»ç¡çœ ãƒ»ä½“æ¸© ãªã©ï¼‰
#  - è¡€åœ§ãƒ»è¡€ä¸­é…¸ç´ é£½å’Œåº¦ï¼ˆSpO2ï¼‰
#  - å…ç–«ã‚¹ã‚³ã‚¢ãƒ»ãƒ›ãƒ«ãƒ¢ãƒ³ï¼ˆã‚¨ã‚¹ãƒˆãƒ­ã‚²ãƒ³/ã‚³ãƒ«ãƒã‚¾ãƒ¼ãƒ«ï¼‰ãƒ‡ãƒ¼ã‚¿
#
# ã“ã‚Œã‚‰ã‚’çµ±åˆçš„ã«ãƒ¢ãƒ‡ãƒ«ã¸å…¥åŠ›ã™ã‚‹ã“ã¨ã§ã€å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚ˆã‚Šã‚‚ç²¾åº¦ã®é«˜ã„
# å¥åº·çŠ¶æ…‹äºˆæ¸¬ãƒ»ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚’ç›®æŒ‡ã™ã€‚
# =====================================================================

from fastapi import FastAPI, HTTPException, Request, BackgroundTasks
from pydantic import BaseModel
import pandas as pd
import json
import os
from datetime import datetime
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
import joblib
import aiofiles
from typing import List, Any
import asyncio
import logging
from typing import Any
import pandas as pd

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
app = FastAPI()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    filename="app.log",
    filemode='a'
)

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)  # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚INFOãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’å‡ºåŠ›
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logging.getLogger().addHandler(console_handler)

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å®šç¾©ã€€â¡ï¸
HEALTH_DATA_FILE = "health_data.json"
SAMPLE_DATA_FILE = "sample_data.json"
COMBINED_DATA_FILE = "combined_data.json"

# ãƒ­ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
lock = asyncio.Lock()

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«å®Ÿè¡Œ
async def on_app_start():
    await save_sample_data_to_combined()
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    asyncio.run(on_app_start())
# éåŒæœŸé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦å®Ÿè¡Œ
async def main():
    await combine_data()
# éåŒæœŸã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
if __name__ == "__main__":
    asyncio.run(main())
# å®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    # asyncioã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒã™ã§ã«å®Ÿè¡Œä¸­ã§ãªã„å ´åˆã«éåŒæœŸé–¢æ•°ã‚’å‘¼ã³å‡ºã™
    loop = asyncio.get_event_loop()
    if loop.is_running():
        # æ—¢å­˜ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå®Ÿè¡Œä¸­ã®å ´åˆ
        loop.create_task(combine_data())
    else:
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ç›´æ¥å®Ÿè¡Œ
        asyncio.run(combine_data())
    
# â­ï¸HealthDataã‚¯ãƒ©ã‚¹ã®å®šç¾©â­ï¸
class HealthData(BaseModel):
   
    source_type: str            # ãƒ‡ãƒ¼ã‚¿ã®å–å¾—å…ƒï¼ˆä¾‹ï¼šã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«ãƒ‡ãƒã‚¤ã‚¹åãªã©ï¼‰
    age: int                    # å¹´é½¢
    height: float               # èº«é•·
    weight: float               # ä½“é‡
    body_fat: float             # ä½“è„‚è‚ªç‡
    heart_rate: int             # å¿ƒæ‹æ•°
    steps: int                  # æ­©æ•°
    sleep_duration: float       # ç¡çœ æ™‚é–“ï¼ˆæ™‚é–“ï¼‰
    body_temperature: float     # ä½“æ¸©ï¼ˆæ‘‚æ°ï¼‰
    exercise_kcal: float        # æ¶ˆè²»ã‚«ãƒ­ãƒªãƒ¼ï¼ˆkcalï¼‰
    systolic_bp: int            # æœ€é«˜è¡€åœ§ï¼ˆåç¸®æœŸï¼‰
    diastolic_bp: int           # æœ€ä½è¡€åœ§ï¼ˆæ‹¡å¼µæœŸï¼‰
    exercise_habit: bool        # é‹å‹•ç¿’æ…£ã®æœ‰ç„¡
    date: str                   # ãƒ‡ãƒ¼ã‚¿å–å¾—æ—¥
    blood_oxygen: float         # è¡€ä¸­é…¸ç´ æ¿ƒåº¦ï¼ˆï¼…ï¼‰

# ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€€â¡ï¸ ç›¸å¯¾ãƒ‘ã‚¹ã«å¤‰æ›´ã™ã‚‹
HEALTH_DATA_FILE = "health_data.json"
COMBINED_DATA_FILE = "combined_data.json"

app = FastAPI()

# ãƒ­ã‚°ã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã® ãƒ­ã‚¬ãƒ¼ï¼ˆloggerï¼‰
# _____________________________________
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# _____________________________________
# æ–°ã—ã„å¥åº·ãƒ‡ãƒ¼ã‚¿ã®é€ä¿¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
from fastapi import FastAPI, HTTPException
import requests
import json
import logging
import os
from datetime import datetime
import pandas as pd

# ===========================================ğŸã‚µãƒ¼ãƒãƒ¼ã«healthdataã‚’ä¿å­˜ğŸ========================================
# ğŸ“„ã‚¢ãƒ—ãƒªã‹ã‚‰ã®æœ€æ–°ãƒ˜ãƒ«ã‚¹ãƒ‡ãƒ¼ã‚¿ã®é€ä¿¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post('/healthdata')
async def submit_healthdata(new_data: dict, background_tasks: BackgroundTasks):
    try:
        background_tasks.add_task(overwrite_health_data, new_data)  # Run this function in the background
        logging.info(f"âœ… æ–°ã—ã„ãƒ˜ãƒ«ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒå—ä¿¡ã•ã‚Œã€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã¨åˆç®—æˆåŠŸğŸ†—: {new_data}")
        return {"message": "æ–°ã—ã„å¥åº·ãƒ‡ãƒ¼ã‚¿ãŒå—ä¿¡ã•ã‚Œã¾ã—ãŸ", "data": new_data}
    except Exception as e:
        logging.error(f"å¥åº·ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail="ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
    except Exception as e:
        logging.error(f"å¥åº·ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail="ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

# ===============================ğŸã€Œã‚¢ãƒ—ãƒªã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦combined_data.jsonã«è¿½åŠ ä¿å­˜ã™ã‚‹ã€ğŸ==========================
import json
import logging
from datetime import datetime
import requests

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)

def fetch_and_update_data_from_api(url, file_path="combined_data.json"):
    """APIã‹ã‚‰ `health data` ã‚’å–å¾—ã—ã€ãƒªã‚¹ãƒˆå‹ã§JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ä¿å­˜å¾Œã€æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™"""
    try:
        # APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        response = requests.get(url)
        response.raise_for_status()  # HTTPã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’æŠ•ã’ã‚‹
        
        data = response.json()  # JSONãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        # `health data` ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°å‡¦ç†ã‚’è¡Œã†
        new_health_data = data.get("health_data", [])
        if new_health_data:
            # `new_health_data` ãŒãƒªã‚¹ãƒˆå‹ã§ãªã„å ´åˆã€ãƒªã‚¹ãƒˆå‹ã«å¤‰æ›
            if not isinstance(new_health_data, list):
                new_health_data = [new_health_data]
            
            # å†…å´ã®äºŒé‡ãƒã‚¹ãƒˆã‚’è§£æ¶ˆã™ã‚‹
            if isinstance(new_health_data[0], dict) and "health_data" in new_health_data[0]:
                new_health_data = new_health_data[0]["health_data"]

            # `combined_data.json` ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ã‚’ç¢ºèª
            try:
                with open(file_path, 'r+', encoding='utf-8') as file:
                    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
                    existing_data = json.load(file)

                    # `health_data` ãŒãƒªã‚¹ãƒˆã§ãªã„å ´åˆã€ãƒªã‚¹ãƒˆã¨ã—ã¦è¨­å®š
                    if "health_data" not in existing_data:
                        existing_data["health_data"] = []
                    
                    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    existing_data["health_data"].extend(new_health_data)  # ãƒªã‚¹ãƒˆå‹ã¨ã—ã¦è¿½åŠ 

                    # ä¸Šæ›¸ãä¿å­˜
                    file.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ã«æˆ»ã™
                    json.dump(existing_data, file, ensure_ascii=False, indent=4)

            except FileNotFoundError:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€æ–°ãŸã«ä½œæˆã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                with open(file_path, 'w', encoding='utf-8') as file:
                    existing_data = {"health_data": new_health_data}  # æ–°ãŸã«ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                    json.dump(existing_data, file, ensure_ascii=False, indent=4)

            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
            with open(file_path, 'r', encoding='utf-8') as file:
                updated_data = json.load(file)
            
            logging.info("âœ… `health_data` ã‚’ãƒªã‚¹ãƒˆå‹ã§è¿½åŠ ä¿å­˜ã—ã€æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            return updated_data
        
        else:
            logging.error("âŒ `health_data` ãŒ API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
    
    except requests.exceptions.RequestException as e:
        logging.error(f"APIãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    except json.JSONDecodeError as e:
        logging.error(f"JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
    except Exception as e:
        logging.error(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    return None


def overwrite_health_data(new_health_data: list, file_path="combined_data.json"):
    """`health_data` ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ä¿å­˜ã™ã‚‹"""
    try:
        # ç¾åœ¨ã®æ—¥ä»˜ãƒ»æ™‚åˆ»ã‚’å–å¾—
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # æ—¥ä»˜ã¨æ™‚é–“ã‚’å–å¾—ï¼ˆä¾‹: 2025-04-01 14:30:00ï¼‰

        # æ—¢å­˜ã® JSON ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                data = json.load(file)
        except FileNotFoundError:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã€æ–°è¦ä½œæˆ
            data = {"health_data": []}

        # `new_health_data` ãŒãƒªã‚¹ãƒˆå‹ã§ãªã„å ´åˆã€ãƒªã‚¹ãƒˆã«å¤‰æ›
        if not isinstance(new_health_data, list):
            logging.warning(f"âš  `new_health_data` ãŒãƒªã‚¹ãƒˆå‹ã§ãªã„ãŸã‚ã€ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™: {new_health_data}")
            new_health_data = [new_health_data]

        # `health_data` ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
        for entry in new_health_data:
            entry["date"] = current_time  # å„ãƒ‡ãƒ¼ã‚¿ã« `date` ã‚’è¿½åŠ 

        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        data["health_data"].extend(new_health_data)  # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 

        # æ›´æ–°å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        with open(file_path, "w", encoding="utf-8") as file:
            json.dump(data, file, ensure_ascii=False, indent=4)

        logging.info(f"âœ… æœ€æ–°ã® `health_data` ã¨ `date` ã‚’ combined_data.json ã«è¿½åŠ ä¿å­˜ğŸ†— ({current_time})")

        # è¿½åŠ å¾Œã«ãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿
        return load_combined_data(file_path)

    except Exception as e:
        logging.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def load_combined_data(file_path="combined_data.json"):
    """combined_data.json ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™"""
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            data = json.load(file)
            return data
    except FileNotFoundError:
        logging.warning(f"âš  JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return None
    except json.JSONDecodeError as e:
        logging.error(f"JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# ===========================================â—ï¸ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã«ã¯å®Ÿè¡Œã—ãªã„â—ï¸=============================================
if __name__ == "__main__":
    logging.info("ğŸ”µ ã‚µãƒ¼ãƒãƒ¼èµ·å‹•: ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“")

    # å¿…è¦ãªã¨ãã« fetch_and_update_data_from_api(url) ã‚’æ‰‹å‹•å®Ÿè¡Œ
    # ä¾‹: fetch_and_update_data_from_api("http://192.168.0.59:8000/healthdata")
 
# =======================ğŸ è§£æã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒªã‚¢ã€€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«Support Vector Regressionï¼ˆSVRï¼‰ã€€ğŸ===========================
import json
import logging
import numpy as np
import os
from sklearn.svm import SVR
from datetime import datetime
import joblib
from sklearn.preprocessing import StandardScaler

TRACK_FILE = "model_last_trained.json"


def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def should_retrain(model_key: str, current_data_count: int, threshold: int = 50) -> bool:
    if not os.path.exists(TRACK_FILE):
        return True
    with open(TRACK_FILE, 'r', encoding='utf-8') as f:
        log = json.load(f)
    prev_count = log.get(model_key, {}).get("data_count", 0)
    return (current_data_count - prev_count) >= threshold


def update_training_log(model_key: str, current_data_count: int):
    if os.path.exists(TRACK_FILE):
        with open(TRACK_FILE, 'r', encoding='utf-8') as f:
            log = json.load(f)
    else:
        log = {}
    log[model_key] = {
        "data_count": current_data_count,
        "last_trained": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    with open(TRACK_FILE, 'w', encoding='utf-8') as f:
        json.dump(log, f, indent=4, ensure_ascii=False)


def load_combined_data(file_path="combined_data.json"):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            combined_data = json.load(file)
            logging.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã‚’èª­ã¿è¾¼ã¿æˆåŠŸï¼")

            if not combined_data:
                logging.error("âŒ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
                return None

            estrogen_data = combined_data.get("Estrogen Data", [])
            cortisol_data = combined_data.get("Cortisol Data", [])
            immunity_data = combined_data.get("Immunity Data", [])
            health_data = combined_data.get("Health Data", {})

            return process_health_data(estrogen_data, cortisol_data, immunity_data, health_data)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logging.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def smooth_data(data, alpha=0.2):
    smoothed = [data[0]]
    for i in range(1, len(data)):
        smoothed.append(alpha * data[i] + (1 - alpha) * smoothed[i - 1])
    return np.array(smoothed)


def generate_health_features(health_data, menstrual_cycle_phase=None):
    height = health_data.get("height", 170)
    weight = health_data.get("weight", 60)
    body_fat = health_data.get("body_fat", 20)
    current_heart_rate = health_data.get("heart_rate", 70)
    steps = health_data.get("steps", 5000)
    sleep_hours = health_data.get("sleep_duration", 7)
    systolic_bp = health_data.get("systolic_bp", 120)
    diastolic_bp = health_data.get("diastolic_bp", 80)
    age = health_data.get("age", 30)
    exercise_habit = int(health_data.get("exercise_habit", False))
    spo2 = health_data.get("spo2", 98)

    bmi = weight / ((height / 100) ** 2)
    blood_pressure_ratio = systolic_bp / max(diastolic_bp, 1)
    exercise_index = exercise_habit * (steps / 10000)

    return np.array([
        height * 0.8,
        weight * 0.8,
        body_fat * 1.2,
        current_heart_rate * 1.1,
        steps * 1.0,
        sleep_hours * 1.2,
        systolic_bp * 1.1,
        diastolic_bp * 1.1,
        age * 1.5,
        bmi * 1.3,
        blood_pressure_ratio * 1.2,
        exercise_index * 1.3,
        spo2 * 0.9
    ]).reshape(1, -1)


def load_or_train_model(X_train, y_train, model_path, scaler_path, model_key="default_model", **kwargs):
    current_data_count = len(y_train)

    if os.path.exists(model_path) and os.path.exists(scaler_path) and not should_retrain(model_key, current_data_count):
        logging.info(f"âœ… {model_key} ãƒ¢ãƒ‡ãƒ«ã¯å†å­¦ç¿’ä¸è¦ã€‚æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return joblib.load(model_path), joblib.load(scaler_path)

    logging.info(f"ğŸ” {model_key} ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ä¸­... ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {current_data_count}")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_train)
    model = SVR(**kwargs)
    model.fit(X_scaled, y_train)
    joblib.dump(model, model_path)
    joblib.dump(scaler, scaler_path)
    update_training_log(model_key, current_data_count)
    return model, scaler


def process_health_data(estrogen_data, cortisol_data, immunity_data, health_data, menstrual_cycle_phase=None):
    logging.info("âœ… å¥åº·ãƒ‡ãƒ¼ã‚¿è§£æé–‹å§‹")
    results = {}
    health_features = generate_health_features(health_data, menstrual_cycle_phase)
    immunity_values = np.array([data.get('Immunity Score', 0) for data in immunity_data]).reshape(-1, 1)

    estrogen_pred = cortisol_pred = immunity_pred = 0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§å›ºå®š

    if len(estrogen_data) > 5:
        estrogen_values = smooth_data(np.array([data['Estrogen'] for data in estrogen_data]))
        if menstrual_cycle_phase == 'ovulation':
            estrogen_values *= 1.5
        elif menstrual_cycle_phase == 'menstruation':
            estrogen_values *= 0.5

        X_estrogen = np.hstack((estrogen_values.reshape(-1, 1), immunity_values, np.tile(health_features, (len(estrogen_values), 1))))
        model_estrogen, scaler_estrogen = load_or_train_model(X_estrogen, estrogen_values.reshape(-1), "model_estrogen.joblib", "scaler_estrogen.joblib", model_key="estrogen", kernel='linear', C=10, epsilon=0.1)
        X_test_e = scaler_estrogen.transform(np.hstack((estrogen_values[-1].reshape(1, -1), immunity_values[-1].reshape(1, -1), health_features)))
        estrogen_pred = model_estrogen.predict(X_test_e)[0]
        results["estrogen_Level"] = round(estrogen_pred, 1)
    else:
        results["estrogen_Level"] = "âš  ãƒ‡ãƒ¼ã‚¿ä¸è¶³"

    if len(cortisol_data) > 1:
        cortisol_values = np.array([data['Cortisol'] for data in cortisol_data]).reshape(-1, 1)
        est_feature = np.full((len(cortisol_values), 1), estrogen_pred)
        X_cortisol = np.hstack((cortisol_values, immunity_values, est_feature, np.tile(health_features, (len(cortisol_values), 1))))
        model_cortisol, scaler_cortisol = load_or_train_model(X_cortisol, cortisol_values.reshape(-1), "model_cortisol.joblib", "scaler_cortisol.joblib", model_key="cortisol", kernel='rbf', C=100, gamma='auto', epsilon=0.1)
        X_test_c = scaler_cortisol.transform(np.hstack((cortisol_values[-1].reshape(1, -1), immunity_values[-1].reshape(1, -1), [[estrogen_pred]], health_features)))
        cortisol_pred = model_cortisol.predict(X_test_c)[0]
        results["cortisol_Level"] = round(cortisol_pred, 1)
    else:
        results["cortisol_Level"] = "âš  ãƒ‡ãƒ¼ã‚¿ä¸è¶³"

    if immunity_values.size > 1:
        cort_feature = np.full((len(immunity_values), 1), cortisol_pred)
        X_immunity = np.hstack((immunity_values, cort_feature, np.tile(health_features, (len(immunity_values), 1))))
        model_immunity, scaler_immunity = load_or_train_model(X_immunity, immunity_values.reshape(-1), "model_immunity.joblib", "scaler_immunity.joblib", model_key="immunity", kernel='rbf', C=100, gamma='auto', epsilon=0.1)
        X_test_i = scaler_immunity.transform(np.hstack((immunity_values[-1].reshape(1, -1), [[cortisol_pred]], health_features)))
        immunity_pred = model_immunity.predict(X_test_i)[0]
        results["immunity_Score"] = round(immunity_pred, 1)
    else:
        results["immunity_Score"] = "âš  ãƒ‡ãƒ¼ã‚¿ä¸è¶³"

    save_analysis_results(results)
    return results


def save_analysis_results(results):
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    results["timestamp"] = current_time
    filename = "analysis_results.json"
    with open(filename, 'w', encoding='utf-8') as file:
        json.dump(results, file, indent=4, ensure_ascii=False)
    logging.info(f"âœ… è§£æçµæœã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    print("\nğŸ‰ è§£ææˆåŠŸ ğŸ‰ï¼çµæœ:")
    for key, value in results.items():
        print(f"{key}: {value}")
    print("ğŸ‰ è§£ææˆåŠŸã—ã¾ã—ãŸ ğŸ‰")


if __name__ == "__main__":
    setup_logging()
    load_combined_data()

# =========================================ğŸ“„ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã‚¢ãƒ—ãƒªã«ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™ğŸ“„==========================================
# âœ…è§£æçµæœã‚’å–å¾—
from fastapi import FastAPI, HTTPException, Query
from datetime import datetime

def fetch_real_health_data(date: str):
    """
    analysis_results.json ã‹ã‚‰è©²å½“ã™ã‚‹æ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚Šã€è‹±èªã‚­ãƒ¼ã«å¤‰æ›ã—ã¦è¿”ã™
    """
    import json

    # analysis_results.jsonã‚’èª­ã¿è¾¼ã‚€
    with open("analysis_results.json", "r", encoding="utf-8") as f:
        data = json.load(f)

    # æ—¥ä»˜ä¸€è‡´ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    if data.get("timestamp", "").startswith(date):
        return {
            "immunity_Score": data.get("immunity_Score"),
            "estrogen_Level": data.get("estrogen_Level"),
            "cortisol_Level": data.get("cortisol_Level"),
            "timestamp": data.get("timestamp")
        }
    else:
        raise ValueError(f"{date} ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

def analyze_health_data(date: str):
    try:
        health_data = fetch_real_health_data(date)

        return {
            "date": date,
            "immunity_Score": health_data["immunity_Score"],
            "estrogen_Level": health_data["estrogen_Level"],
            "cortisol_Level": health_data["cortisol_Level"],
            "timestamp": health_data.get("ä¿å­˜æ—¥æ™‚", datetime.now().isoformat())
        }

    except Exception as e:
        print(f"analyze_health_data å†…ã§ã‚¨ãƒ©ãƒ¼: {e}")
        raise e

@app.get('/healthdata')
async def get_health_data(date: str = Query(None, description="å–å¾—ã™ã‚‹æ—¥ä»˜ (YYYY-MM-DD)")):
    """
    æŒ‡å®šã—ãŸæ—¥ä»˜ã®å¥åº·ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    """
    try:
        if not date:
            raise HTTPException(status_code=400, detail="æ—¥ä»˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")

        # `analyze_health_data` ã®å®Ÿè¡Œ
        result = analyze_health_data(date)
        
        if not result:
            raise HTTPException(status_code=404, detail="ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        return result
    except HTTPException as e:
        raise e  # FastAPI ã® HTTPException ã¯ãã®ã¾ã¾é€å‡º
    except Exception as e:
        print(f"get_health_data å†…ã§ã‚¨ãƒ©ãƒ¼: {e}")  # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
        raise HTTPException(status_code=500, detail=f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")

# âœ…ã‚¨ã‚¹ãƒˆãƒ­ã‚²ãƒ³ç”¨
@app.get('/analyze_health_data/estrogen')
async def estrogen_data(date: str = Query(None, description="å–å¾—ã™ã‚‹æ—¥ä»˜ (YYYY-MM-DD)")):
    """
    ã‚¨ã‚¹ãƒˆãƒ­ã‚²ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    """
    try:
        if not date:
            raise HTTPException(status_code=400, detail="æ—¥ä»˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
        result = analyze_health_data(date)
        estrogen_level = result.get("estrogen_Level", "ãƒ‡ãƒ¼ã‚¿ä¸è¶³")  # ä¿®æ­£ç®‡æ‰€: estrogenLevel -> estrogen_Level
        return {"estrogen_Level": estrogen_level}  # ä¿®æ­£ç®‡æ‰€: estrogenLevel -> estrogen_Level
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")

# âœ…ã‚³ãƒ«ãƒã‚¾ãƒ¼ãƒ«ç”¨
@app.get('/analyze_health_data/cortisol')
async def cortisol_data(date: str = Query(None, description="å–å¾—ã™ã‚‹æ—¥ä»˜ (YYYY-MM-DD)")):
    """
    ã‚³ãƒ«ãƒã‚¾ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    """
    try:
        if not date:
            raise HTTPException(status_code=400, detail="æ—¥ä»˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
        result = analyze_health_data(date)
        cortisol_level = result.get("cortisol_Level", "ãƒ‡ãƒ¼ã‚¿ä¸è¶³")  # ä¿®æ­£ç®‡æ‰€: cortisolLevel -> cortisol_Level
        return {"cortisol_Level": cortisol_level}  # ä¿®æ­£ç®‡æ‰€: cortisolLevel -> cortisol_Level
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")

# âœ…å…ç–«åŠ›ç”¨
@app.get('/analyze_health_data/immunity')
async def immunity_data(date: str = Query(None, description="å–å¾—ã™ã‚‹æ—¥ä»˜ (YYYY-MM-DD)")):
    """
    å…ç–«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    """
    try:
        if not date:
            raise HTTPException(status_code=400, detail="æ—¥ä»˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
        result = analyze_health_data(date)
        immunity_score = result.get("immunity_Score", "ãƒ‡ãƒ¼ã‚¿ä¸è¶³")  # ä¿®æ­£ç®‡æ‰€: immunityScore -> immunity_Score
        return {"immunity_Score": immunity_score}  # ä¿®æ­£ç®‡æ‰€: immunityScore -> immunity_Score
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")

# âœ…ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç”¨
@app.get('/analyze_health_data/calendar')
async def get_analyzed_health_data(date: str = Query(None, description="å–å¾—ã™ã‚‹æ—¥ä»˜ (YYYY-MM-DD)")):
    result = analyze_health_data(date)
    return result
    
    """
    è§£æãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‚’å–å¾—
    """
    try:
        if not date:
            raise HTTPException(status_code=400, detail="æ—¥ä»˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
        result = analyze_health_data(date)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")

# =========================================ğŸ›œã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å‡¦ç†ï¼†èµ·å‹•ğŸ›œ=================================================
@app.on_event("startup")
async def startup_event():
    logging.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•å‡¦ç†ãŒå®Œäº†ï¼ã‚¹ã‚¿ãƒ¼ãƒˆã§ãã¾ã™ï¼ ğŸ†—")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, debug=True)


# ===========================================       å‚™è€ƒæ¬„ã€€ã€€ã€€ã€€ã€€ã€€ã€€==================================================
# trueæœ‰åŠ¹ã€€Falseç„¡åŠ¹
# http://localhost:5000/healthdata ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨ã€ä»®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
# http://192.168.0.59:8000/healthdata ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨ã€å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

# é–‹ç™ºç”¨ãƒ†ã‚¹ãƒˆå®Œäº†å¾Œã€æœ¬ç•ªå…¬é–‹ç”¨ã«å¤‰æ›´ã™ã‚‹ã€€ã€ŒWSGIã‚µãƒ¼ãƒãƒ¼ã€

# â¬‡ï¸ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã‚¢ãƒ—ãƒªã«ãƒ‡ãƒ¼ã‚¿ãŒæ¸¡ã‚ŒãŸã‹ç¢ºèªã§ãã¾ã™â¬‡ï¸
# http://192.168.0.59:8000/analyze_health_data/estrogen?date=2025-04-   ç¶šãã®æ—¥ä»˜ã‚’å…¥åŠ›
# http://192.168.0.59:8000/analyze_health_data/cortisol?date=2025-04-ã€€ã€€ç¶šãã®æ—¥ä»˜ã‚’å…¥åŠ›
# http://192.168.0.59:8000/analyze_health_data/immunity?date=2025-04-  ç¶šãã®æ—¥ä»˜ã‚’å…¥åŠ›
# http://192.168.0.59:8000/analyze_health_data/calendar?date=2025-04-ã€€ ç¶šãã®æ—¥ä»˜ã‚’å…¥åŠ›
