# ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼

# ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€â­ï¸ãƒ‡ãƒ¼ã‚¿è§£æã‚·ã‚¹ãƒ†ãƒ  æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€ŒHanakoğŸ˜¸ã€â­ï¸

# ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼

from fastapi import FastAPI, HTTPException, Request, BackgroundTasks
from pydantic import BaseModel
import pandas as pd
import json
import os
from datetime import datetime, timedelta, timezone
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
import joblib
import aiofiles
from typing import List, Any
import asyncio
import logging
from typing import Any
import pandas as pd

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
app = FastAPI()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    filename="app.log",
    filemode='a'
)

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)  # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚INFOãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’å‡ºåŠ›
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logging.getLogger().addHandler(console_handler)

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å®šç¾©
HEALTH_DATA_FILE = "health_data.json"
SAMPLE_DATA_FILE = "sample_data.json"
COMBINED_DATA_FILE = "combined_data.json"

# ãƒ­ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
lock = asyncio.Lock()

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«å®Ÿè¡Œ
async def on_app_start():
    await save_sample_data_to_combined()
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    asyncio.run(on_app_start())
# éåŒæœŸé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦å®Ÿè¡Œ
async def main():
    await combine_data()
# éåŒæœŸã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
if __name__ == "__main__":
    asyncio.run(main())
# å®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    # asyncioã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒã™ã§ã«å®Ÿè¡Œä¸­ã§ãªã„å ´åˆã«éåŒæœŸé–¢æ•°ã‚’å‘¼ã³å‡ºã™
    loop = asyncio.get_event_loop()
    if loop.is_running():
        # æ—¢å­˜ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå®Ÿè¡Œä¸­ã®å ´åˆ
        loop.create_task(combine_data())
    else:
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ç›´æ¥å®Ÿè¡Œ
        asyncio.run(combine_data())
    
# â­ï¸HealthDataã‚¯ãƒ©ã‚¹ã®å®šç¾©â­ï¸
class HealthData(BaseModel):
   
    source_type: str            # ãƒ‡ãƒ¼ã‚¿ã®å–å¾—å…ƒï¼ˆä¾‹ï¼šã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«ãƒ‡ãƒã‚¤ã‚¹åãªã©ï¼‰
    age: int                    # å¹´é½¢
    height: float               # èº«é•·
    weight: float               # ä½“é‡
    body_fat: float             # ä½“è„‚è‚ªç‡
    heart_rate: int             # å¿ƒæ‹æ•°
    steps: int                  # æ­©æ•°
    sleep_duration: float       # ç¡çœ æ™‚é–“ï¼ˆæ™‚é–“ï¼‰
    body_temperature: float     # ä½“æ¸©ï¼ˆæ‘‚æ°ï¼‰
    exercise_kcal: float        # æ¶ˆè²»ã‚«ãƒ­ãƒªãƒ¼ï¼ˆkcalï¼‰
    systolic_bp: int            # æœ€é«˜è¡€åœ§ï¼ˆåç¸®æœŸï¼‰
    diastolic_bp: int           # æœ€ä½è¡€åœ§ï¼ˆæ‹¡å¼µæœŸï¼‰
    exercise_habit: bool        # é‹å‹•ç¿’æ…£ã®æœ‰ç„¡
    date: str                   # ãƒ‡ãƒ¼ã‚¿å–å¾—æ—¥
    blood_oxygen: float         # è¡€ä¸­é…¸ç´ æ¿ƒåº¦ï¼ˆï¼…ï¼‰

# ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
HEALTH_DATA_FILE = "/path/to/health_data.json"
COMBINED_DATA_FILE = "/path/to/combined_data.json"

# ãƒ­ã‚°ã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã® ãƒ­ã‚¬ãƒ¼ï¼ˆloggerï¼‰
# _____________________________________
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# _____________________________________
# æ–°ã—ã„å¥åº·ãƒ‡ãƒ¼ã‚¿ã®é€ä¿¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
from fastapi import FastAPI, HTTPException
import requests
import json
import logging
import os
from datetime import datetime
import pandas as pd

# ===========================================ğŸã‚µãƒ¼ãƒãƒ¼ã«healthdataã‚’ä¿å­˜ğŸ========================================
# ğŸ“„ã‚¢ãƒ—ãƒªã‹ã‚‰ã®æœ€æ–°ãƒ˜ãƒ«ã‚¹ãƒ‡ãƒ¼ã‚¿ã®é€ä¿¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post('/healthdata')
async def submit_healthdata(new_data: dict, background_tasks: BackgroundTasks):
    try:
        background_tasks.add_task(overwrite_health_data, new_data)
        logging.info(f"âœ… æ–°ã—ã„ãƒ˜ãƒ«ã‚¹ãƒ‡ãƒ¼ã‚¿å—ä¿¡: {new_data}")
        return {"message": "æ–°ã—ã„å¥åº·ãƒ‡ãƒ¼ã‚¿ãŒå—ä¿¡ã•ã‚Œã¾ã—ãŸ", "data": new_data}
    except Exception as e:
        logging.error(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail="ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

# ===============================ğŸã€Œã‚¢ãƒ—ãƒªã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦combined_data.jsonã«è¿½åŠ ä¿å­˜ã™ã‚‹ã€ğŸ==========================
import json
import logging
from datetime import datetime
import requests

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)

def fetch_and_update_data_from_api(url, file_path="combined_data.json"):
    """APIã‹ã‚‰ `health data` ã‚’å–å¾—ã—ã€ãƒªã‚¹ãƒˆå‹ã§JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ä¿å­˜å¾Œã€æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™"""
    try:
        # APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        response = requests.get(url)
        response.raise_for_status()  # HTTPã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’æŠ•ã’ã‚‹
        
        data = response.json()  # JSONãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        # `health data` ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°å‡¦ç†ã‚’è¡Œã†
        new_health_data = data.get("health_data", [])
        if new_health_data:
            # `new_health_data` ãŒãƒªã‚¹ãƒˆå‹ã§ãªã„å ´åˆã€ãƒªã‚¹ãƒˆå‹ã«å¤‰æ›
            if not isinstance(new_health_data, list):
                new_health_data = [new_health_data]
            
            # å†…å´ã®äºŒé‡ãƒã‚¹ãƒˆã‚’è§£æ¶ˆã™ã‚‹
            if isinstance(new_health_data[0], dict) and "health_data" in new_health_data[0]:
                new_health_data = new_health_data[0]["health_data"]

            # `combined_data.json` ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ã‚’ç¢ºèª
            try:
                with open(file_path, 'r+', encoding='utf-8') as file:
                    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
                    existing_data = json.load(file)

                    # `health_data` ãŒãƒªã‚¹ãƒˆã§ãªã„å ´åˆã€ãƒªã‚¹ãƒˆã¨ã—ã¦è¨­å®š
                    if "health_data" not in existing_data:
                        existing_data["health_data"] = []
                    
                    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    existing_data["health_data"].extend(new_health_data)  # ãƒªã‚¹ãƒˆå‹ã¨ã—ã¦è¿½åŠ 

                    # ä¸Šæ›¸ãä¿å­˜
                    file.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ã«æˆ»ã™
                    json.dump(existing_data, file, ensure_ascii=False, indent=4)

            except FileNotFoundError:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€æ–°ãŸã«ä½œæˆã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                with open(file_path, 'w', encoding='utf-8') as file:
                    existing_data = {"health_data": new_health_data}  # æ–°ãŸã«ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                    json.dump(existing_data, file, ensure_ascii=False, indent=4)

            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
            with open(file_path, 'r', encoding='utf-8') as file:
                updated_data = json.load(file)
            
            logging.info("âœ… `health_data` ã‚’ãƒªã‚¹ãƒˆå‹ã§è¿½åŠ ä¿å­˜ã—ã€æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            return updated_data
        
        else:
            logging.error("âŒ `health_data` ãŒ API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
    
    except requests.exceptions.RequestException as e:
        logging.error(f"APIãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    except json.JSONDecodeError as e:
        logging.error(f"JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
    except Exception as e:
        logging.error(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    return None


def overwrite_health_data(new_health_data: list, file_path="combined_data.json"):
    """`health_data` ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ä¿å­˜ã™ã‚‹"""
    try:
        # ç¾åœ¨ã®æ—¥ä»˜ãƒ»æ™‚åˆ»ã‚’å–å¾—
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # æ—¥ä»˜ã¨æ™‚é–“ã‚’å–å¾—ï¼ˆä¾‹: 2025-04-01 14:30:00ï¼‰

        # æ—¢å­˜ã® JSON ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                data = json.load(file)
        except FileNotFoundError:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã€æ–°è¦ä½œæˆ
            data = {"health_data": []}

        # `new_health_data` ãŒãƒªã‚¹ãƒˆå‹ã§ãªã„å ´åˆã€ãƒªã‚¹ãƒˆã«å¤‰æ›
        if not isinstance(new_health_data, list):
            logging.warning(f"âš  `new_health_data` ãŒãƒªã‚¹ãƒˆå‹ã§ãªã„ãŸã‚ã€ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™: {new_health_data}")
            new_health_data = [new_health_data]

        # `health_data` ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
        for entry in new_health_data:
            entry["date"] = current_time  # å„ãƒ‡ãƒ¼ã‚¿ã« `date` ã‚’è¿½åŠ 

        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        data["health_data"].extend(new_health_data)  # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 

        # æ›´æ–°å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        with open(file_path, "w", encoding="utf-8") as file:
            json.dump(data, file, ensure_ascii=False, indent=4)

        logging.info(f"âœ… æœ€æ–°ã® `health_data` ã¨ `date` ã‚’ combined_data.json ã«è¿½åŠ ä¿å­˜ğŸ†— ({current_time})")

        # è¿½åŠ å¾Œã«ãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿
        return load_combined_data(file_path)

    except Exception as e:
        logging.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def load_combined_data(file_path="combined_data.json"):
    """combined_data.json ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™"""
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            data = json.load(file)
            return data
    except FileNotFoundError:
        logging.warning(f"âš  JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return None
    except json.JSONDecodeError as e:
        logging.error(f"JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# ===========================================â—ï¸ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã«ã¯å®Ÿè¡Œã—ãªã„â—ï¸=============================================
if __name__ == "__main__":
    logging.info("ğŸ”µ ã‚µãƒ¼ãƒãƒ¼èµ·å‹•: ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“")

 
# =======================ğŸ è§£æã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒªã‚¢ã€€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«Support Vector Regressionï¼ˆSVRï¼‰ã€€ğŸ===========================
import json
import logging
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVR
from datetime import datetime
import joblib
from sklearn.preprocessing import StandardScaler

def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_combined_data(file_path="combined_data.json"):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            combined_data = json.load(file)
            logging.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã‚’èª­ã¿è¾¼ã¿æˆåŠŸï¼")
            
            if not combined_data:
                logging.error("âŒ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
                return None
            
            estrogen_data = combined_data.get("Estrogen Data", [])
            cortisol_data = combined_data.get("Cortisol Data", [])
            immunity_data = combined_data.get("Immunity Data", [])
            health_data = combined_data.get("Health Data", {})
            
            return process_health_data(estrogen_data, cortisol_data, immunity_data, health_data)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logging.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def smooth_data(data, alpha=0.2):
    smoothed = [data[0]]
    for i in range(1, len(data)):
        smoothed.append(alpha * data[i] + (1 - alpha) * smoothed[i-1])
    return np.array(smoothed)

def process_health_data(estrogen_data, cortisol_data, immunity_data, health_data, menstrual_cycle_phase=None):
    logging.info("âœ… å¥åº·ãƒ‡ãƒ¼ã‚¿è§£æé–‹å§‹")
    
    try:
        # å¥åº·ãƒ‡ãƒ¼ã‚¿å–å¾— & ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        height = health_data.get("height", 170)
        weight = health_data.get("weight", 60)
        body_fat = health_data.get("body_fat", 20)
        current_heart_rate = health_data.get("heart_rate", 70)
        steps = health_data.get("steps", 5000)
        sleep_hours = health_data.get("sleep_duration", 7)
        systolic_bp = health_data.get("systolic_bp", 120)
        diastolic_bp = health_data.get("diastolic_bp", 80)
        age = health_data.get("age", 30)
        exercise_habit = int(health_data.get("exercise_habit", False))
        spo2 = health_data.get("spo2", 98)

        # æœˆçµŒå‘¨æœŸã®å½±éŸ¿è€ƒæ…®
        estrogen_multiplier = 1
        if menstrual_cycle_phase:
            if menstrual_cycle_phase == 'ovulation':
                estrogen_multiplier = 1.5
            elif menstrual_cycle_phase == 'menstruation':
                estrogen_multiplier = 0.5
        
        # BMI, è¡€åœ§æ¯”, é‹å‹•æŒ‡æ•°ã®è¨ˆç®—
        bmi = weight / ((height / 100) ** 2)
        blood_pressure_ratio = systolic_bp / max(diastolic_bp, 1)
        exercise_index = exercise_habit * (steps / 10000)  # é‹å‹•ç¿’æ…£ã®æŒ‡æ•°åŒ–

        # ç‰¹å¾´é‡ã®é‡ã¿ä»˜ã‘èª¿æ•´
        health_features = np.array([
            height * 0.8,  # èº«é•·
            weight * 0.8,  # ä½“é‡
            body_fat * 1.2,  # ä½“è„‚è‚ªç‡ï¼ˆé‡è¦è¦–ï¼‰1.5~1.8
            current_heart_rate * 1.1,  # å¿ƒæ‹æ•°ï¼ˆé‡è¦è¦–ï¼‰1.3~1.5
            steps * 1.0,  # æ­©æ•°
            sleep_hours * 1.2,  # ç¡çœ æ™‚é–“ï¼ˆå…ç–«ã¨ã®é–¢ä¿‚ãŒæ·±ã„ãŸã‚ï¼‰1.2 â†’ 1.5
            systolic_bp * 1.1,  # åç¸®æœŸè¡€åœ§
            diastolic_bp * 1.1,  # æ‹¡å¼µæœŸè¡€åœ§
            age * 1.5,  # å¹´é½¢ï¼ˆå¼·èª¿ï¼‰1.8~2.0
            bmi * 1.3,  # BMIï¼ˆå¥åº·æŒ‡æ¨™ã¨ã—ã¦é‡è¦ï¼‰
            blood_pressure_ratio * 1.2,  # è¡€åœ§æ¯”
            exercise_index * 1.3,  # é‹å‹•æŒ‡æ•°ï¼ˆé‹å‹•ã®å½±éŸ¿ã‚’å¼·èª¿ï¼‰1.5~1.7
            spo2 * 0.9  # SpOâ‚‚ï¼ˆé€šå¸¸å€¤ãŒå®‰å®šã—ã¦ã„ã‚‹ãŸã‚å½±éŸ¿ã¯å°ã•ã‚ï¼‰ 0.7~0.8
        ]).reshape(1, -1)

        scaler = StandardScaler()
        immunity_values = np.array([data.get('Immunity Score', 0) for data in immunity_data]).reshape(-1, 1)

        results = {}

        # ã‚¨ã‚¹ãƒˆãƒ­ã‚²ãƒ³æ¨å®š
        if len(estrogen_data) > 5:
            estrogen_values = smooth_data(np.array([data['Estrogen'] for data in estrogen_data]), alpha=0.2)
            estrogen_values *= estrogen_multiplier
            X_train = np.hstack((estrogen_values.reshape(-1, 1), immunity_values, np.tile(health_features, (len(estrogen_values), 1))))
            X_train = scaler.fit_transform(X_train)
            model_estrogen = SVR(kernel='linear', C=10, epsilon=0.1)
            model_estrogen.fit(X_train, estrogen_values.reshape(-1))
            X_test = scaler.transform(np.hstack((estrogen_values[-1].reshape(1, -1), immunity_values[-1].reshape(1, -1), health_features)))
            results["estrogen_Level"] = round(model_estrogen.predict(X_test)[0], 1)
        else:
            results["estrogen_Level"] = "âš  ãƒ‡ãƒ¼ã‚¿ä¸è¶³"

        # ã‚³ãƒ«ãƒã‚¾ãƒ¼ãƒ«æ¨å®š
        if len(cortisol_data) > 1:
            cortisol_values = np.array([data['Cortisol'] for data in cortisol_data]).reshape(-1, 1)
            X_train = np.hstack((cortisol_values, immunity_values, np.tile(health_features, (len(cortisol_values), 1))))
            X_train = scaler.fit_transform(X_train)
            model_cortisol = SVR(kernel='rbf', C=100, gamma='auto', epsilon=0.1)
            model_cortisol.fit(X_train, cortisol_values.reshape(-1))
            X_test = scaler.transform(np.hstack((cortisol_values[-1].reshape(1, -1), immunity_values[-1].reshape(1, -1), health_features)))
            results["cortisol_Level"] = round(float(model_cortisol.predict(X_test)[0]), 1)
        else:
            results["cortisol_Level"] = "âš  ãƒ‡ãƒ¼ã‚¿ä¸è¶³"

        # å…ç–«ã‚¹ã‚³ã‚¢æ¨å®š
        if immunity_values.size > 1:
            X_train = np.hstack((immunity_values, np.tile(health_features, (len(immunity_values), 1))))
            X_train = scaler.fit_transform(X_train)
            model_immunity = SVR(kernel='rbf', C=100, gamma='auto', epsilon=0.1)
            model_immunity.fit(X_train, immunity_values.reshape(-1))
            X_test = scaler.transform(np.hstack((immunity_values[-1].reshape(1, -1), health_features)))
            results["immunity_Score"] = round(model_immunity.predict(X_test)[0], 1)
        else:
            results["immunity_Score"] = "âš  ãƒ‡ãƒ¼ã‚¿ä¸è¶³"

        save_analysis_results(results)
        return results

    except Exception as e:
        logging.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        return {"ã‚¨ãƒ©ãƒ¼": str(e)}

# è§£æçµæœã®ãƒ‡ãƒ¼ã‚¿ã‚’githubã®analysis_resultsã«ä¿å­˜ã™ã‚‹ã€€2025/7/28å¤‰æ›´ï¼

DATE = datetime.now().strftime('%Y-%m-%d')
FILE_PATH = f"analysis_results/{DATE}.json"

# è¿½åŠ 
import os
import json
import base64
import requests
from datetime import datetime

def save_analysis_results(results):
    # timestampã‚’è¿½åŠ 
    results["timestamp"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")  # ç’°å¢ƒå¤‰æ•°ã«GitHubãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¦ãŠã
    REPO = "monta-marin/takeshi_y"
    BRANCH = "main"
    DATE = datetime.now().strftime('%Y-%m-%d')
    FILE_PATH = f"analysis_results/{DATE}.json"

    json_str = json.dumps(results, ensure_ascii=False, indent=2)
    b64_content = base64.b64encode(json_str.encode("utf-8")).decode("utf-8")

    url = f"https://api.github.com/repos/{REPO}/contents/{FILE_PATH}"
    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }

    resp = requests.get(url, headers=headers)
    sha = resp.json().get("sha") if resp.status_code == 200 else None

    payload = {
        "message": f"Add analysis result for {DATE}",
        "content": b64_content,
        "branch": BRANCH
    }
    if sha:
        payload["sha"] = sha

    put_resp = requests.put(url, headers=headers, json=payload)
    if put_resp.status_code in [200, 201]:
        print(f"âœ… GitHubã«ä¿å­˜æˆåŠŸ: {FILE_PATH}")
    else:
        print(f"âŒ GitHubä¿å­˜å¤±æ•—: {put_resp.status_code}")
        print(put_resp.json())


if __name__ == "__main__":
    setup_logging()
    load_combined_data()

# =========================================ğŸ“„ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã‚¢ãƒ—ãƒªã«ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™ğŸ“„==========================================
# âœ…è§£æçµæœã‚’å–å¾—
from fastapi import FastAPI, HTTPException, Query
from datetime import datetime
import json
import os

@app.get("/")
def read_root():
    return {"message": "FastAPI is running"}


def validate_date_format(date_str: str):
    try:
        datetime.strptime(date_str, "%Y-%m-%d")
    except ValueError:
        raise HTTPException(status_code=400, detail="æ—¥ä»˜å½¢å¼ãŒä¸æ­£ã§ã™ã€‚YYYY-MM-DD å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")


def fetch_real_health_data(date: str):
    """
    æŒ‡å®šæ—¥ä»˜ã® JSON ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€å¥åº·ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
    """
    file_path = f"analysis_results/{date}.json"

    if not os.path.isfile(file_path):
        raise ValueError(f"{file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise ValueError(f"{file_path} ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    required_keys = ["immunity_Score", "estrogen_Level", "cortisol_Level", "timestamp"]
    for key in required_keys:
        if key not in data:
            raise ValueError(f"{key} ãŒ {file_path} ã«å­˜åœ¨ã—ã¾ã›ã‚“")

    return {
        "immunity_Score": data["immunity_Score"],
        "estrogen_Level": data["estrogen_Level"],
        "cortisol_Level": data["cortisol_Level"],
        "timestamp": data["timestamp"]
    }


def analyze_health_data(date: str):
    """
    å¥åº·ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€æ•´å½¢ã•ã‚ŒãŸè¾æ›¸ã‚’è¿”ã™
    
    file_path = f"analyze_results/{date}.json"

    """
    try:
        validate_date_format(date)
        health_data = fetch_real_health_data(date)
        return {
            "date": date,
            "immunity_Score": health_data["immunity_Score"],
            "estrogen_Level": health_data["estrogen_Level"],
            "cortisol_Level": health_data["cortisol_Level"],
            "timestamp": health_data["timestamp"]
        }
    except Exception as e:
        print(f"[ERROR] analyze_health_data: {e}")
        raise e

# âœ… ç·åˆãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç­‰ã§ä½¿ç”¨ï¼‰
@app.get("/healthdata/calendar")
async def get_analyzed_health_data(date: str = Query(..., description="å–å¾—ã™ã‚‹æ—¥ä»˜ (YYYY-MM-DD)")):
    try:
        return analyze_health_data(date)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")


# âœ… ã‚¨ã‚¹ãƒˆãƒ­ã‚²ãƒ³å–å¾—
@app.get("/healthdata/estrogen")
async def estrogen_data(date: str = Query(..., description="å–å¾—ã™ã‚‹æ—¥ä»˜ (YYYY-MM-DD)")):
    try:
        result = analyze_health_data(date)
        return {"estrogen_Level": result.get("estrogen_Level", "ãƒ‡ãƒ¼ã‚¿ä¸è¶³")}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")


# âœ… ã‚³ãƒ«ãƒã‚¾ãƒ¼ãƒ«å–å¾—
@app.get("/healthdata/cortisol")
async def cortisol_data(date: str = Query(..., description="å–å¾—ã™ã‚‹æ—¥ä»˜ (YYYY-MM-DD)")):
    try:
        result = analyze_health_data(date)
        return {"cortisol_Level": result.get("cortisol_Level", "ãƒ‡ãƒ¼ã‚¿ä¸è¶³")}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")


# âœ… å…ç–«ã‚¹ã‚³ã‚¢å–å¾—
@app.get("/healthdata/immunity")
async def immunity_data(date: str = Query(..., description="å–å¾—ã™ã‚‹æ—¥ä»˜ (YYYY-MM-DD)")):
    try:
        result = analyze_health_data(date)
        return {"immunity_Score": result.get("immunity_Score", "ãƒ‡ãƒ¼ã‚¿ä¸è¶³")}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")


# âœ… ä»»æ„æ—¥ä»˜ã®å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆGETç”¨ï¼‰
@app.get("/healthdata")
async def get_health_data(date: str = Query(..., description="å–å¾—ã™ã‚‹æ—¥ä»˜ (YYYY-MM-DD)")):
    try:
        result = analyze_health_data(date)
        return result
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        print(f"[ERROR] get_health_data: {e}")
        raise HTTPException(status_code=500, detail=f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")

# =========================================ğŸ›œã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å‡¦ç†ï¼†èµ·å‹•ğŸ›œ=================================================
from fastapi import FastAPI, Request
import logging
import uvicorn
import os
import asyncio
import json
from datetime import datetime

# ----------------- ãƒ­ã‚°è¨­å®š -----------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# ----------------- FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ -----------------
app = FastAPI()

# ----------------- ãƒ‡ãƒ¼ã‚¿çµåˆãƒ»æ›´æ–°å‡¦ç† -----------------
COMBINED_DATA_FILE = "combined_data.json"

async def combine_data():
    """
    èµ·å‹•æ™‚ã«å‘¼ã°ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿çµåˆå‡¦ç†ã€‚
    å¿…è¦ã«å¿œã˜ã¦ fetch/update é–¢æ•°ã‚’å‘¼ã¶ã“ã¨ãŒã§ãã¾ã™ã€‚
    """
    logging.info("combine_data is running (placeholder)")
    
    # ä¾‹: combined_data.json ãŒãªã‘ã‚Œã°ä½œæˆ
    try:
        with open(COMBINED_DATA_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
    except FileNotFoundError:
        logging.info(f"{COMBINED_DATA_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
        data = {"health_data": []}
        with open(COMBINED_DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    
    # å®Ÿéš›ã«ã“ã“ã§ overwrite_health_data ã‚„ fetch_and_update_data_from_api ã‚’å‘¼ã¶ã“ã¨ãŒå¯èƒ½
    # await overwrite_health_data(new_data) ãªã©
    await asyncio.sleep(0.1)  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼éåŒæœŸå‡¦ç†

# ----------------- èµ·å‹•æ™‚å‡¦ç† -----------------
@app.on_event("startup")
async def startup_event():
    logging.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•å‡¦ç†ãŒå®Œäº†ï¼ã‚¹ã‚¿ãƒ¼ãƒˆã§ãã¾ã™ï¼ ğŸ†—")
    # ãƒ‡ãƒ¼ã‚¿çµåˆå‡¦ç†ã‚’å‘¼ã¶
    await combine_data()

# ----------------- ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ -----------------
@app.get("/")
def read_root():
    return {"message": "FastAPI is running on Cloud Run!"}

# ----------------- ãƒ‡ãƒ¼ã‚¿å—ä¿¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ -----------------
@app.post("/send-data")
async def send_data(request: Request):
    data = await request.json()
    logging.info(f"å—ä¿¡ãƒ‡ãƒ¼ã‚¿: {data}")

    # å—ä¿¡ãƒ‡ãƒ¼ã‚¿ã‚’ combined_data.json ã«è¿½è¨˜ã™ã‚‹ä¾‹
    try:
        with open(COMBINED_DATA_FILE, "r", encoding="utf-8") as f:
            combined = json.load(f)
    except FileNotFoundError:
        combined = {"health_data": []}

    # æ—¥ä»˜ã‚’è¿½åŠ 
    data["date"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    combined["health_data"].append(data)

    with open(COMBINED_DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(combined, f, ensure_ascii=False, indent=4)

    return {"status": "success", "received": data}

# ----------------- ã‚¢ãƒ—ãƒªèµ·å‹• -----------------
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    uvicorn.run("takeshi:app", host="0.0.0.0", port=port)



# ===========================================       å‚™è€ƒæ¬„ã€€ã€€ã€€ã€€ã€€ã€€ã€€==================================================
# trueæœ‰åŠ¹ã€€Falseç„¡åŠ¹
# http://localhost:5000/healthdata ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨ã€ä»®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
# http://192.168.0.59:8000/healthdata ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨ã€å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

# é–‹ç™ºç”¨ãƒ†ã‚¹ãƒˆå®Œäº†å¾Œã€æœ¬ç•ªå…¬é–‹ç”¨ã«å¤‰æ›´ã™ã‚‹ã€€ã€ŒWSGIã‚µãƒ¼ãƒãƒ¼ã€
